{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW1 - Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import usefull libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Understanding of MDPs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 9, 4, 5, 7, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CID = \"1594572\"\n",
    "#CID = \"12345678\"\n",
    "CID = np.array([int(CID[i]) for i in range(len(CID))])\n",
    "CID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 2, 0, 2, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.mod(CID + 1 ,3)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.mod(CID[1:] ,2)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau = np.array([ [s[i],r[i]] for i in range(len(r))]).flatten()\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.5\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "x = CID[-3]\n",
    "y = CID[-2]\n",
    "z = CID[-1]\n",
    "\n",
    "#Generation of personalized Data :\n",
    "j = np.mod(z+1,3)+1\n",
    "print(j)\n",
    "p= 0.25 + 0.5 *x/10.\n",
    "print(p)\n",
    "gamma = 0.0 #0.3+0.5* y /10.\n",
    "print(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Understanding of Grid Worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(2,0),(3,0),(3,1),(3,1),(3,3),(1,2)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,0),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100 ] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,3)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [ p, (1-p)/3, (1-p)/3 , (1-p)/3]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "    def optimal_value_policy(self, threshold, discount):\n",
    "        # Computes the Optimal Value and Policy with the Value Itteration Algorithm\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(self.state_size)\n",
    "        # The policy is initialised at 0\n",
    "        policy = np.zeros((self.state_size,), dtype=np.int)\n",
    "        \n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(self.state_size):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # List variable for the Value of a state\n",
    "                tmpQ_list = []\n",
    "                for action_idx in range(self.action_size):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] *(R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpQ_list.append(tmpQ)\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                 # Choosing the largest valued Policy (greedy)\n",
    "\n",
    "                policy[state_idx] = int(np.argmax(tmpQ_list))\n",
    "                Vnew[state_idx] = tmpQ_list[policy[state_idx]]\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "                \n",
    "        return policy, V\n",
    "        \n",
    "###########################################         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myWorld = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "optPolicy, optV = myWorld.optimal_value_policy(0.0001,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0. ,   4.5,  -1. ,  -1. ,   4.5,  -1. ,  -1. ,  -1. , -17.5,\n",
       "        -1. ,   0. ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myWorld.draw_deterministic_policy(optPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ic_std]",
   "language": "python",
   "name": "conda-env-ic_std-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
