{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CW1 - Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import usefull libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Understanding of MDPs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 9, 4, 5, 7, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CID = \"1594572\"\n",
    "#CID = \"12345678\"\n",
    "CID = np.array([int(CID[i]) for i in range(len(CID))])\n",
    "CID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, 2, 0, 2, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = np.mod(CID + 1 ,3)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = np.mod(CID[1:] ,2)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau = np.array([ [s[i],r[i]] for i in range(len(r))]).flatten()\n",
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.5\n",
      "0.6499999999999999\n"
     ]
    }
   ],
   "source": [
    "x = CID[-3]\n",
    "y = CID[-2]\n",
    "z = CID[-1]\n",
    "\n",
    "#Generation of personalized Data :\n",
    "j = np.mod(z+1,3)+1\n",
    "print(j)\n",
    "p= 0.25 + 0.5 *x/10.\n",
    "print(p)\n",
    "gamma = 0.3+0.5* y /10.\n",
    "print(gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Understanding of Grid Worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        ### Attributes defining the Gridworld #######\n",
    "        # Shape of the gridworld\n",
    "        self.shape = (4,4)\n",
    "        \n",
    "        # Locations of the obstacles\n",
    "        self.obstacle_locs = [(2,0),(3,0),(3,1),(3,1),(3,3),(1,2)]\n",
    "        \n",
    "        # Locations for the absorbing states\n",
    "        self.absorbing_locs = [(0,0),(3,2)]\n",
    "        \n",
    "        # Rewards for each of the absorbing states \n",
    "        self.special_rewards = [10, -100 ] #corresponds to each of the absorbing_locs\n",
    "        \n",
    "        # Reward for all the other states\n",
    "        self.default_reward = -1\n",
    "        \n",
    "        # Starting location\n",
    "        self.starting_loc = (0,3)\n",
    "        \n",
    "        # Action names\n",
    "        self.action_names = ['N','E','S','W']\n",
    "        \n",
    "        # Number of actions\n",
    "        self.action_size = len(self.action_names)\n",
    "        \n",
    "        \n",
    "        # Randomizing action results: [1 0 0 0] to no Noise in the action results.\n",
    "        self.action_randomizing_array = [ p, (1-p)/3, (1-p)/3 , (1-p)/3]\n",
    "        \n",
    "        ############################################\n",
    "    \n",
    "    \n",
    "    \n",
    "        #### Internal State  ####\n",
    "        \n",
    "    \n",
    "        # Get attributes defining the world\n",
    "        state_size, T, R, absorbing, locs = self.build_grid_world()\n",
    "        \n",
    "        # Number of valid states in the gridworld (there are 11 of them)\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # Transition operator (3D tensor)\n",
    "        self.T = T\n",
    "        \n",
    "        # Reward function (3D tensor)\n",
    "        self.R = R\n",
    "        \n",
    "        # Absorbing states\n",
    "        self.absorbing = absorbing\n",
    "        \n",
    "        # The locations of the valid states\n",
    "        self.locs = locs\n",
    "        \n",
    "        # Number of the starting state\n",
    "        self.starting_state = self.loc_to_state(self.starting_loc, locs);\n",
    "        \n",
    "        # Locating the initial state\n",
    "        self.initial = np.zeros((1,len(locs)))\n",
    "        self.initial[0,self.starting_state] = 1\n",
    "        \n",
    "        \n",
    "        # Placing the walls on a bitmap\n",
    "        self.walls = np.zeros(self.shape);\n",
    "        for ob in self.obstacle_locs:\n",
    "            self.walls[ob]=1\n",
    "            \n",
    "        # Placing the absorbers on a grid for illustration\n",
    "        self.absorbers = np.zeros(self.shape)\n",
    "        for ab in self.absorbing_locs:\n",
    "            self.absorbers[ab] = -1\n",
    "        \n",
    "        # Placing the rewarders on a grid for illustration\n",
    "        self.rewarders = np.zeros(self.shape)\n",
    "        for i, rew in enumerate(self.absorbing_locs):\n",
    "            self.rewarders[rew] = self.special_rewards[i]\n",
    "        \n",
    "        #Illustrating the grid world\n",
    "        self.paint_maps()\n",
    "        ################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ####### Getters ###########\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        return self.T\n",
    "    \n",
    "    def get_reward_matrix(self):\n",
    "        return self.R\n",
    "    \n",
    "    \n",
    "    ########################\n",
    "    \n",
    "    ####### Methods #########\n",
    "    def policy_evaluation(self, policy, threshold, discount):\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(policy.shape[0])\n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(policy.shape[0]):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # Accumulator variable for the Value of a state\n",
    "                tmpV = 0\n",
    "                for action_idx in range(policy.shape[1]):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(policy.shape[0]):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] * (R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpV += policy[state_idx,action_idx] * tmpQ\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                Vnew[state_idx] = tmpV\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "            \n",
    "        return V\n",
    "    \n",
    "    def draw_deterministic_policy(self, Policy):\n",
    "        # Draw a deterministic policy\n",
    "        # The policy needs to be a np array of 22 values between 0 and 3 with\n",
    "        # 0 -> N, 1->E, 2->S, 3->W\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.imshow(self.walls+self.rewarders +self.absorbers)\n",
    "        #plt.hold('on')\n",
    "        for state, action in enumerate(Policy):\n",
    "            if(self.absorbing[0,state]):\n",
    "                continue\n",
    "            arrows = [r\"$\\uparrow$\",r\"$\\rightarrow$\", r\"$\\downarrow$\", r\"$\\leftarrow$\"]\n",
    "            action_arrow = arrows[action]\n",
    "            location = self.locs[state]\n",
    "            plt.text(location[1], location[0], action_arrow, ha='center', va='center')\n",
    "    \n",
    "        plt.show()\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ########### Internal Helper Functions #####################\n",
    "    def paint_maps(self):\n",
    "        plt.figure()\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(self.walls)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(self.absorbers)\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(self.rewarders)\n",
    "        plt.show()\n",
    "        \n",
    "    def build_grid_world(self):\n",
    "        # Get the locations of all the valid states, the neighbours of each state (by state number),\n",
    "        # and the absorbing states (array of 0's with ones in the absorbing states)\n",
    "        locations, neighbours, absorbing = self.get_topology()\n",
    "        \n",
    "        # Get the number of states\n",
    "        S = len(locations)\n",
    "        \n",
    "        # Initialise the transition matrix\n",
    "        T = np.zeros((S,S,4))\n",
    "        \n",
    "        for action in range(4):\n",
    "            for effect in range(4):\n",
    "                \n",
    "                # Randomize the outcome of taking an action\n",
    "                outcome = (action+effect+1) % 4\n",
    "                if outcome == 0:\n",
    "                    outcome = 3\n",
    "                else:\n",
    "                    outcome -= 1\n",
    "    \n",
    "                # Fill the transition matrix\n",
    "                prob = self.action_randomizing_array[effect]\n",
    "                for prior_state in range(S):\n",
    "                    post_state = neighbours[prior_state, outcome]\n",
    "                    post_state = int(post_state)\n",
    "                    T[post_state,prior_state,action] = T[post_state,prior_state,action]+prob\n",
    "                    \n",
    "    \n",
    "        # Build the reward matrix\n",
    "        R = self.default_reward*np.ones((S,S,4))\n",
    "        for i, sr in enumerate(self.special_rewards):\n",
    "            post_state = self.loc_to_state(self.absorbing_locs[i],locations)\n",
    "            R[post_state,:,:]= sr\n",
    "        \n",
    "        return S, T,R,absorbing,locations\n",
    "    \n",
    "    def get_topology(self):\n",
    "        height = self.shape[0]\n",
    "        width = self.shape[1]\n",
    "        \n",
    "        index = 1 \n",
    "        locs = []\n",
    "        neighbour_locs = []\n",
    "        \n",
    "        for i in range(height):\n",
    "            for j in range(width):\n",
    "                # Get the locaiton of each state\n",
    "                loc = (i,j)\n",
    "                \n",
    "                #And append it to the valid state locations if it is a valid state (ie not absorbing)\n",
    "                if(self.is_location(loc)):\n",
    "                    locs.append(loc)\n",
    "                    \n",
    "                    # Get an array with the neighbours of each state, in terms of locations\n",
    "                    local_neighbours = [self.get_neighbour(loc,direction) for direction in ['nr','ea','so', 'we']]\n",
    "                    neighbour_locs.append(local_neighbours)\n",
    "                \n",
    "        # translate neighbour lists from locations to states\n",
    "        num_states = len(locs)\n",
    "        state_neighbours = np.zeros((num_states,4))\n",
    "        \n",
    "        for state in range(num_states):\n",
    "            for direction in range(4):\n",
    "                # Find neighbour location\n",
    "                nloc = neighbour_locs[state][direction]\n",
    "                \n",
    "                # Turn location into a state number\n",
    "                nstate = self.loc_to_state(nloc,locs)\n",
    "      \n",
    "                # Insert into neighbour matrix\n",
    "                state_neighbours[state,direction] = nstate;\n",
    "                \n",
    "    \n",
    "        # Translate absorbing locations into absorbing state indices\n",
    "        absorbing = np.zeros((1,num_states))\n",
    "        for a in self.absorbing_locs:\n",
    "            absorbing_state = self.loc_to_state(a,locs)\n",
    "            absorbing[0,absorbing_state] =1\n",
    "        \n",
    "        return locs, state_neighbours, absorbing \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def loc_to_state(self,loc,locs):\n",
    "        #takes list of locations and gives index corresponding to input loc\n",
    "        return locs.index(tuple(loc))\n",
    "\n",
    "\n",
    "    def is_location(self, loc):\n",
    "        # It is a valid location if it is in grid and not obstacle\n",
    "        if(loc[0]<0 or loc[1]<0 or loc[0]>self.shape[0]-1 or loc[1]>self.shape[1]-1):\n",
    "            return False\n",
    "        elif(loc in self.obstacle_locs):\n",
    "            return False\n",
    "        else:\n",
    "             return True\n",
    "            \n",
    "    def get_neighbour(self,loc,direction):\n",
    "        #Find the valid neighbours (ie that are in the grif and not obstacle)\n",
    "        i = loc[0]\n",
    "        j = loc[1]\n",
    "        \n",
    "        nr = (i-1,j)\n",
    "        ea = (i,j+1)\n",
    "        so = (i+1,j)\n",
    "        we = (i,j-1)\n",
    "        \n",
    "        # If the neighbour is a valid location, accept it, otherwise, stay put\n",
    "        if(direction == 'nr' and self.is_location(nr)):\n",
    "            return nr\n",
    "        elif(direction == 'ea' and self.is_location(ea)):\n",
    "            return ea\n",
    "        elif(direction == 'so' and self.is_location(so)):\n",
    "            return so\n",
    "        elif(direction == 'we' and self.is_location(we)):\n",
    "            return we\n",
    "        else:\n",
    "            #default is to return to the same location\n",
    "            return loc\n",
    "        \n",
    "    def optimal_value_policy(self, threshold, discount):\n",
    "        # Computes the Optimal Value and Policy with the Value Itteration Algorithm\n",
    "        \n",
    "        # Make sure delta is bigger than the threshold to start with\n",
    "        delta= 2*threshold\n",
    "        \n",
    "        #Get the reward and transition matrices\n",
    "        R = self.get_reward_matrix()\n",
    "        T = self.get_transition_matrix()\n",
    "        \n",
    "        # The value is initialised at 0\n",
    "        V = np.zeros(self.state_size)\n",
    "        # The policy is initialised at 0\n",
    "        policy = np.zeros((self.state_size,), dtype=np.int)\n",
    "        \n",
    "        # Make a deep copy of the value array to hold the update during the evaluation\n",
    "        Vnew = np.copy(V)\n",
    "        \n",
    "        # While the Value has not yet converged do:\n",
    "        while delta>threshold:\n",
    "            for state_idx in range(self.state_size):\n",
    "                # If it is one of the absorbing states, ignore\n",
    "                if(self.absorbing[0,state_idx]):\n",
    "                    continue   \n",
    "                \n",
    "                # List variable for the Value of a state\n",
    "                tmpQ_list = []\n",
    "                for action_idx in range(self.action_size):\n",
    "                    # Accumulator variable for the State-Action Value\n",
    "                    tmpQ = 0\n",
    "                    for state_idx_prime in range(self.state_size):\n",
    "                        tmpQ = tmpQ + T[state_idx_prime,state_idx,action_idx] *(R[state_idx_prime,state_idx, action_idx] + discount * V[state_idx_prime])\n",
    "                    \n",
    "                    tmpQ_list.append(tmpQ)\n",
    "                    \n",
    "                # Update the value of the state\n",
    "                 # Choosing the largest valued Policy (greedy)\n",
    "\n",
    "                policy[state_idx] = int(np.argmax(tmpQ_list))\n",
    "                Vnew[state_idx] = tmpQ_list[policy[state_idx]]\n",
    "            \n",
    "            # After updating the values of all states, update the delta\n",
    "            delta =  max(abs(Vnew-V))\n",
    "            # and save the new value into the old\n",
    "            V=np.copy(Vnew)\n",
    "                \n",
    "        return policy, V\n",
    "        \n",
    "###########################################         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myWorld = GridWorld()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optPolicy, optV = myWorld.optimal_value_policy(0.0001,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 3, 3, 0, 3, 0, 0, 3, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   5.28366338,   0.73191542,  -1.32774645,\n",
       "         5.91316542,   1.21829509,  -2.56530113,  -3.76482201,\n",
       "       -21.64655263,  -5.33465824,   0.        ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUUAAAEzCAYAAACmDxGBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAFAVJREFUeJzt3H+s3XWd5/Hni7a0Uhz5ObYpxeLazIgsCN40qJsN8UdSkYAZYMNsZoCJ5MbZddVk3CzOJDhDZrM67sDExQypIwqjo8yCPypCDGiNjIYfhS1QLK6VbKRDXWbo0FpwCvW+9497XD89nNIf53vOudx5PpKT+/3xOd/Pu597vq/7/XG+TVUhSZp1xKQLkKS5xFCUpIahKEkNQ1GSGoaiJDUMRUlqDBWKSY5LcmeSH/V+Hrufdr9Isqn3Wj9Mn5I0Shnme4pJ/gzYUVUfS3IlcGxV/ZcB7XZX1dFD1ClJYzFsKP4QOKeqtidZDnynqn5jQDtDUdLLwrDXFF9dVdsBej9/fT/tliTZmOSeJO8Zsk9JGpmFB2qQ5C5g2YBVf3QI/ZxcVU8meS3w7SSPVNWPB/Q1DUwDLD0qb/rN1x15CF3Mb8/6OKZ00B575Pl/rKoTD+e9Yzl97nvP54DbquqWl2o3dcaSuu+bKw+7tvnmgT3PT7oE6WVjzaqfPFBVU4fz3mFPn9cDl/WmLwO+1t8gybFJFvemTwDeCvxgyH4laSSGDcWPAe9M8iPgnb15kkwl+atem9cDG5M8BGwAPlZVhqKkOemA1xRfSlU9Dbx9wPKNwBW96e8D/3qYfiRpXHyiRZIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIanYRikrVJfphka5IrB6xfnOTm3vp7k6zqol9J6trQoZhkAfAp4F3AqcBvJzm1r9l7gX+qqtcB1wIfH7ZfSRqFLo4U1wBbq+rxqnoe+BJwQV+bC4Abe9O3AG9Pkg761hjMzBR33/XcpMuYMxyPfc238egiFFcATzTz23rLBrapqr3ATuD4DvrWiM3MFFd/eAeb7t8z6VLmBMdjX/NxPBZ2sI1BR3x1GG1IMg1MA5y8oovSNKxbP7+bO77yLKesXsT3NmzfZ93KVQv5xLoTJ1TZZDge+5qP49FF8mwDVjbzJwFP7qfNtiQLgVcBO/o3VFXrgHUAU2cseVFoavzefeFSvvWN5zjv4qWcd9HRky5n4hyPfc3H8eji9Pl+YHWSU5IcCVwCrO9rsx64rDd9EfDtqjL0XgaOWnoE1372RJ7ZMTPpUuYEx2Nf83E80kU2JTkX+AtgAXBDVf3XJFcDG6tqfZIlwF8DZzJ7hHhJVT3+UtucOmNJ3ffNlS/V5F+UB/Y8P+kSpJeNNat+8kBVTR3Oezu5cFdVtwO39y27qpn+Z+DiLvqSpFHyiRZJahiKktQwFCWpYShKUsNQlKSGoShJDUNRkhqGoiQ1DEVJahiKktQwFCWpYShKUsNQlKSGoShJDUNRkhqGoiQ1DEVJahiKktQwFCWpYShKUsNQlKSGoShJDUNRkhqGoiQ1DEVJanQSiknWJvlhkq1Jrhyw/vIk/5BkU+91RRf9SlLXFg67gSQLgE8B7wS2AfcnWV9VP+hrenNVvX/Y/iRplLo4UlwDbK2qx6vqeeBLwAUdbFeSxq6LUFwBPNHMb+st63dhkoeT3JJkZQf9SlLnhj59BjJgWfXNfx34YlXtSfI+4EbgbS/aUDINTAMsW7GAB/Y830F5mo8WZWbSJcwpL5T3TLvSxUhuA9ojv5OAJ9sGVfV0Ve3pzX4aeNOgDVXVuqqaqqqpY45b0EFpknRougjF+4HVSU5JciRwCbC+bZBkeTN7PrClg34lqXNDnz5X1d4k7we+CSwAbqiqR5NcDWysqvXAB5KcD+wFdgCXD9uvJI1CF9cUqarbgdv7ll3VTH8E+EgXfUnSKHl1VpIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIanYRikhuSPJVk837WJ8knk2xN8nCSs7rodxx275rhsc3PT7oM6WVhPuwvXR0pfg5Y+xLr3wWs7r2mgb/sqN+R2r1rhg9c+hRX/NZP+f6Gn0+6HGlOmy/7SyehWFXfBXa8RJMLgJtq1j3AMUmWd9H3KP3plU9z2lmLmXrLEq7/82f46d/vnXRJEzEzU9x913OTLkNz3HzZX8Z1TXEF8EQzv623bE7742uOZ+17juLY4xfw6VuWsWzFwkmXNHYzM8XVH97Bpvv3TLoUzXHzZX8ZV9UZsKxe1CiZZvb0mmUrFoy6pgNasuRXfzMWLxn0T5j/bv38bu74yrOcsnoR39uwfZ91K1ct5BPrTpxQZZpr5sv+Mq5Q3AasbOZPAp7sb1RV64B1AK8/ffGLQlPj9+4Ll/KtbzzHeRcv5byLjp50OdLIjev0eT1wae8u9NnAzqrafqA3afKOWnoE1372RJ7ZMTPpUqSx6ORIMckXgXOAE5JsAz4KLAKoquuB24Fzga3Ac8DvddGvxuMVRx3B70z/2qTLkMYiVXPzLPX1py+um76+bNJlaI5aFI9cWy+Uz2G01qz6yQNVNXU473UkJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJanRSSgmuSHJU0k272f9OUl2JtnUe13VRb+S1LWFHW3nc8B1wE0v0ebuqjqvo/4kaSQ6OVKsqu8CO7rYliRN0jivKb45yUNJ7kjyhjH2K0kHravT5wN5EHhNVe1Oci7wVWB1f6Mk08A0wPIVC1iUmTGVN/e9UN4TazkeGpWxfLKqaldV7e5N3w4sSnLCgHbrqmqqqqaOOc4PvaTxG0vyJFmWJL3pNb1+nx5H35J0KDo5fU7yReAc4IQk24CPAosAqup64CLg95PsBX4OXFJV1UXfktSlTkKxqn77AOuvY/YrO5I0p3nhTpIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIaQ4dikpVJNiTZkuTRJB8c0CZJPplka5KHk5w1bL+ajN27Znhs8/OTLkNz1Hz4fHRxpLgX+IOqej1wNvAfk5za1+ZdwOreaxr4yw761Zjt3jXDBy59iit+66d8f8PPJ12O5pj58vkYOhSrantVPdib/hmwBVjR1+wC4KaadQ9wTJLlw/at8frTK5/mtLMWM/WWJVz/58/w07/fO+mSJmJmprj7rucmXcacM18+H51eU0yyCjgTuLdv1QrgiWZ+Gy8OTs1xf3zN8ax9z1Ece/wCPn3LMpatWDjpksZuZqa4+sM72HT/nkmXMufMl89HZ1UnORq4FfhQVe3qXz3gLTVgG9PMnl6zfMWCrkpTR5Ys+dXf0MVLBv1K579bP7+bO77yLKesXsT3NmzfZ93KVQv5xLoTJ1TZ5M2Xz0cnoZhkEbOB+IWq+vKAJtuAlc38ScCT/Y2qah2wDuDU0498UWhKk/buC5fyrW88x3kXL+W8i46edDkagS7uPgf4DLClqq7ZT7P1wKW9u9BnAzuravt+2kpz1lFLj+Daz57IMztmJl2KRqSLI8W3Ar8LPJJkU2/ZHwInA1TV9cDtwLnAVuA54Pc66FeaiFccdQS/M/1rky5DI5KquXmWeurpR9bf3PbqSZcxZ7xQfs9eOlhrVv3kgaqaOpz3uqdJUsNQlKSGoShJDUNRkhqGoiQ1DEVJahiKktQwFCWpYShKUsNQlKSGoShJDUNRkhqGoiQ1DEVJahiKktQwFCWpYShKUsNQlKSGoShJDUNRkhqGoiQ1DEVJahiKktQwFCWpYShKUmPoUEyyMsmGJFuSPJrkgwPanJNkZ5JNvddVw/YrSaOwsINt7AX+oKoeTPJK4IEkd1bVD/ra3V1V53XQnySNzNBHilW1vaoe7E3/DNgCrBh2u5I0CZ1eU0yyCjgTuHfA6jcneSjJHUne0GW/ktSVLk6fAUhyNHAr8KGq2tW3+kHgNVW1O8m5wFeB1QO2MQ1MAyxfsaCr0uaFRZmZdAlzyn9edfakS5hTPvF/7pl0CfNGJ0eKSRYxG4hfqKov96+vql1Vtbs3fTuwKMkJA9qtq6qpqpo65jhvjEsavy7uPgf4DLClqq7ZT5tlvXYkWdPr9+lh+5akrnVx+vxW4HeBR5Js6i37Q+BkgKq6HrgI+P0ke4GfA5dUVXXQtyR1auhQrKq/A3KANtcB1w3blySNmhfuJKlhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpSw1CUpIahKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJakxdCgmWZLkviQPJXk0yZ8MaLM4yc1Jtia5N8mqYfuVpFHo4khxD/C2qjoDeCOwNsnZfW3eC/xTVb0OuBb4eAf9SlLnhg7FmrW7N7uo96q+ZhcAN/ambwHeniTD9i1JXevkmmKSBUk2AU8Bd1bVvX1NVgBPAFTVXmAncHwXfUtSlzoJxar6RVW9ETgJWJPktL4mg44K+48mSTKdZGOSjc/smOmiNEk6JJ3efa6qZ4DvAGv7Vm0DVgIkWQi8Ctgx4P3rqmqqqqaOOc4b45LGr4u7zycmOaY3/QrgHcBjfc3WA5f1pi8Cvl1VLzpSlKRJW9jBNpYDNyZZwGzI/m1V3ZbkamBjVa0HPgP8dZKtzB4hXtJBv5LUuaFDsaoeBs4csPyqZvqfgYuH7UuSRs0Ld5LUMBQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUMBQlqWEoSlLDUJSkhqEoSQ1DUZIahqIkNQxFSWoYipLUGDoUkyxJcl+Sh5I8muRPBrS5PMk/JNnUe10xbL+SNAoLO9jGHuBtVbU7ySLg75LcUVX39LW7uare30F/kjQyQ4diVRWwuze7qPeqYbcrSZPQyTXFJAuSbAKeAu6sqnsHNLswycNJbkmysot+JalrmT3Q62hjyTHAV4D/VFWbm+XHA7urak+S9wH/rqreNuD908B0b/Y0YHN/mwk4AfjHSReBdfSzjn1Zx75+o6peeThv7DQUAZJ8FHi2qv77ftYvAHZU1asOsJ2NVTXVaXGHwTqswzr+ZdXRxd3nE3tHiCR5BfAO4LG+Nsub2fOBLcP2K0mj0MXd5+XAjb0jwCOAv62q25JcDWysqvXAB5KcD+wFdgCXd9CvJHWui7vPDwNnDlh+VTP9EeAjh7jpdUOW1hXr2Jd17Ms69vWyr6Pza4qS9HLmY36S1JgzoZjkuCR3JvlR7+ex+2n3i+ZxwfUd9r82yQ+TbE1y5YD1i5Pc3Ft/b5JVXfV9iHWM5ZHJJDckeSrJwK9FZdYne3U+nOSsCdRwTpKdzVhcNahdB3WsTLIhyZbeo6wfHNBmHONxMHWMfEwO8tHeke8vI3vEuKrmxAv4M+DK3vSVwMf30273CPpeAPwYeC1wJPAQcGpfm/8AXN+bvoTZxxYnUcflwHVj+H38W+AsYPN+1p8L3AEEOBu4dwI1nAPcNoaxWA6c1Zt+JfC/B/xexjEeB1PHyMek9288uje9CLgXOLuvzTj2l4Op45D3lzlzpAhcANzYm74ReM8Y+14DbK2qx6vqeeBLvXr2V98twNuTZAJ1jEVVfZfZbwrszwXATTXrHuCYvq9ejaOGsaiq7VX1YG/6Z8x+pWxFX7NxjMfB1DFyvX/jgR7tHfn+cpB1HLK5FIqvrqrtMPvLB359P+2WJNmY5J4kXQXnCuCJZn4bL/6w/f82VbUX2Akc31H/h1IHzI1HJg+21lF7c+/06Y4kbxh1Z73TwDOZPSppjXU8XqIOGMOY5MCP9o5jfxnJI8ZjDcUkdyXZPOB1KEdDJ9fsN9X/PfAXSf5VF6UNWNb/F+dg2oyjjq8Dq6rqdOAufvXXeNzGMR4H8iDwmqo6A/gfwFdH2VmSo4FbgQ9V1a7+1QPeMpLxOEAdYxmTqvpFVb0ROAlYk+S0/jIHvW0CdRzy/jLWUKyqd1TVaQNeXwP+7y9PN3o/n9rPNp7s/Xwc+A4DviN5GLYB7V+Qk4An99cmyULgVXR/anfAOqrq6ara05v9NPCmjms4WAczZiNVVbt+efpUVbcDi5KcMIq+Mvvf4t0KfKGqvjygyVjG40B1jHNMen08w+x+uLZv1Tj2lwPWcTj7y1w6fV4PXNabvgz4Wn+DJMcmWdybPgF4K/CDDvq+H1id5JQkRzJ7Ybj/znZb30XAt6t3JbdDB6wjc+eRyfXApb27rmcDO395+WNckiz75XWqJGuY/Tw/PYJ+AnwG2FJV1+yn2cjH42DqGMeY5CAe7WUM+8vB1HFY+0vXd4QO98Xs9YZvAT/q/Tyut3wK+Kve9FuAR5i9K/sI8N4O+z+X2bt5Pwb+qLfsauD83vQS4H8CW4H7gNeOaBwOVMd/Ax7tjcEG4DdHVMcXge3AC8z+1X8v8D7gffWrO3+f6tX5CDA1gRre34zFPcBbRjQW/4bZU7+HgU2917kTGI+DqWPkYwKcDvyvXh2bgasGfE5Hvr8cZB2HvL/4RIskNebS6bMkTZyhKEkNQ1GSGoaiJDUMRUlqGIqS1DAUJalhKEpS4/8BO/MOVj7BlhAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "myWorld.draw_deterministic_policy(optPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ic_std]",
   "language": "python",
   "name": "conda-env-ic_std-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
